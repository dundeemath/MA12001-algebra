[
  {
    "objectID": "week6.html",
    "href": "week6.html",
    "title": "Week 6: Applications of matrices, and Gaussian elimination",
    "section": "",
    "text": "\\[\n\\newenvironment{amatrix}[1]{%\n  \\left[\\begin{array}{#1}\n}{%\n  \\end{array}\\right]\n}\n\\]",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 6: Applications of matrices, and Gaussian elimination"
    ]
  },
  {
    "objectID": "week6.html#rotation-matrices",
    "href": "week6.html#rotation-matrices",
    "title": "Week 6: Applications of matrices, and Gaussian elimination",
    "section": "Rotation matrices",
    "text": "Rotation matrices\nThe following matrix rotates vectors around the origin by an angle \\(\\theta\\) anticlockwise: \\[\nR_\\theta = \\begin{bmatrix} \\cos\\theta & -\\sin\\theta \\\\ \\sin\\theta & \\cos\\theta \\end{bmatrix}\n\\] Why? Consider a position vector \\(\\mathbf{a}=\\begin{bmatrix} r\\cos\\alpha \\\\ r\\sin\\alpha \\end{bmatrix}\\). This is \\(\\begin{bmatrix} x \\\\ y \\end{bmatrix}\\) at polar coordinates \\((r,\\alpha)\\), which hopefully you’ve seen before.\nThen applying the matrix, \\[\n{\\mathbf{b}} = R_\\theta \\mathbf{r} = \\begin{bmatrix} r\\cos\\alpha\\cos\\theta - r \\sin\\alpha\\sin\\theta \\\\ r\\cos\\alpha\\sin\\theta + r\\sin\\alpha\\cos\\theta \\end{bmatrix} = \\begin{bmatrix} r\\cos{(\\alpha+\\theta)} \\\\ r\\sin(\\alpha+\\theta) \\end{bmatrix}\n\\] which is the position vector with polar coordinates \\((r,\\alpha+\\theta)\\). This diagram might help: \nThink: What’s the determinant of the rotation matrix \\(R_\\theta\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\cos\\theta\\cos\\theta - (-\\sin\\theta)\\sin\\theta = \\cos^2 \\theta + \\sin^2\\theta = 1\\).",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 6: Applications of matrices, and Gaussian elimination"
    ]
  },
  {
    "objectID": "week6.html#reflection-matrices",
    "href": "week6.html#reflection-matrices",
    "title": "Week 6: Applications of matrices, and Gaussian elimination",
    "section": "Reflection matrices",
    "text": "Reflection matrices\nConsider the matrix \\[\nP=\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}.\n\\] Then \\(P{\\mathbf{x}} = [x_1,-x_2]\\). All it’s done is reversed the \\(y\\) component. In other words, we’ve reflected over the \\(x\\)-axis.\nSimilarly, \\[\nQ=\\begin{bmatrix} -1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\] applied to any vector is just a reflection over the \\(y\\)-axis.\nThink: What’s the determinant of the reflection matrix \\(P\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n$1 - 0 = -1\n\n\n\nAll reflections have determinant \\(-1\\), and all rotations have determinant \\(+1\\). You can think of the determinant as a scale factor for areas: rotations don’t change the area, and reflections flip it.\nTo get a reflection through a different line, you can always use a combination of rotations and reflections.\nThe matrix given by \\[\nR_{\\theta} P R_{-\\theta}\n\\] reflects in the line which makes angle \\(\\theta\\) with the \\(x\\)-axis: first we rotate by \\(-\\theta\\), then reflect over the \\(x\\)-axis, then rotate back.\nMany more transformations exist, such as shear and projection. In 3D they get much more complicated. This sort of matrix is used extensively in the development of video games and computer graphics.",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 6: Applications of matrices, and Gaussian elimination"
    ]
  },
  {
    "objectID": "week6.html#gaussian-elimination",
    "href": "week6.html#gaussian-elimination",
    "title": "Week 6: Applications of matrices, and Gaussian elimination",
    "section": "Gaussian elimination",
    "text": "Gaussian elimination\nWe’ve actually already seen Gaussian elimination, for calculating the inverse of \\(3\\times3\\) matrices. The principle here is the same: perform a series of row operations to simplify the matrix:\n\nStart with a linear system of equations.\nWrite this as a matrix equation \\(A{\\mathbf{x}}={\\mathbf{b}}\\) where \\(A\\) and \\({\\mathbf{b}}\\) are known, and we want to find \\({\\mathbf{x}}\\).\nWrite an augmented matrix \\([A|{\\mathbf{b}}]\\).\nPerform a series of row operations to reduce \\(A\\) to row-echelon form.\nTransform back to a series of equations.\nUse this form to read off the solution.\n\nThese steps should become clear when we see an example. First we need to know what row-echelon form means.\n\nRow-echelon form\nA matrix is in row-echelon form if each row has more zeros at the start than the preceding row (unless they’re all zero). It is always possible to convert a matrix into this form.\nThink: which of the following matrices are in row-echelon form?\n\\[\nA = \\begin{bmatrix} 1 & 2 & 3\\\\0 & 5 & 6\\\\0&0&1 \\end{bmatrix}, B=\\begin{bmatrix} 1 & 0 &0\\\\0 & 0 &0 \\\\ 0 & 0 &0 \\end{bmatrix}, C=\\begin{bmatrix} 1 & 0 & 0\\\\0 & 0 &-4\\\\ 0 & 0 &1 \\end{bmatrix}\n\\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(A\\) is very standard row-echelon form. \\(B\\) is in row echelon form, but \\(C\\) is not because the final row doesn’t have more leading zeros than the previous one.\n\n\n\n\n\nExample 1\nLet’s solve the following system via Gaussian elimination:^{The two 8s in the final equation are deliberate, otherwise it would have been singular}\n\\[\n\\begin{aligned}\nx_1 + 2x_2 + 3x_3 &= 1,\\\\\n4x_1+5x_2+6x_3 &= 2,\\\\\n7x_1+8x_2+8x_3 &= 3.\n\\end{aligned}\n\\] First we rewrite this as a matrix equation: \\[\n\\begin{bmatrix} 1&2&3\\\\4&5&6\\\\7&8&8 \\end{bmatrix} {\\mathbf{x}} = \\begin{bmatrix} 1\\\\2\\\\3 \\end{bmatrix}\n\\] and then this can be represented with an augmented matrix as \\[\n\\begin{amatrix}{ccc|c}\n1&2&3&1\\\\\n4&5&6&2\\\\\n7&8&8&3\n\\end{amatrix}.\n\\] Now let’s put it in row echelon form using row operations, just like last week. We need a zero at the start of the second row: \\[\n\\hookrightarrow\\begin{amatrix}{ccc|c}\n1&2&3&1\\\\\n0&-3&-6&-2\\\\\n7&8&8&3\n\\end{amatrix} \\qquad R_2 \\to R_2 - 4 R_1,\n\\] a zero at the start of the third row: \\[\n\\hookrightarrow\\begin{amatrix}{ccc|c}\n1&2&3&1\\\\\n0&-3&-6&-2\\\\\n0&-6&-13&-4\n\\end{amatrix} \\qquad R_3 \\to R_3 - 7 R_1,\n\\] and another zero at the start of the third row \\[\n\\hookrightarrow\\begin{amatrix}{ccc|c}\n1&2&3&1\\\\\n0&-3&-6&-2\\\\\n0&0&-1&0\n\\end{amatrix} \\qquad R_3 \\to R_3 - 2 R_2,\n\\] This is now in row echelon form. How does this help us? Well, if we write this back as a system of equations we now have \\[\n\\begin{aligned}\nx_1 + 2x_2 + 3x_3 &= 1,\\\\\n-3x_2-6x_3 &= -2,\\\\\n-x_3&=0.\n\\end{aligned}\n\\] So immediately we know that we must have \\(x_3=0\\). Then the second of these equations gives us \\(-3x_2=-2\\), so \\(x_2=\\frac{2}{3}\\), and finally plugging this into the first equation we find \\(x_1=-\\frac{1}{3}\\).\nBasically, this method formalises techniques you may have seen before for solving simultaneous equations. It’s particularly useful because it allows us to deal with singular matrices too:\n\n\nZero rows\nWhat if we perform Gaussian elimination and get to a point where the final row is all zeros? This happens exactly when the initial matrix is singular, and it means we can’t follow the above procedure to find a unique solution to the system. Instead, we either have no solutions or an infinite number of solutions, depending on whether the final row gives us \\(0=0\\) or \\(0=1\\). The second case is impossible, so we have no solutions, whereas if we get \\(0=0\\) then we have only 2 equations for 3 unknowns, so there are infinitely many solutions.\n\n\nExample 2\nSimilar to the previous example, let’s consider: \\[\n\\begin{aligned}\nx_1 + 2x_2 + 3x_3 &= 1,\\\\\n4x_1+5x_2+6x_3 &= 2,\\\\\n7x_1+8x_2+9x_3 &= 3.\n\\end{aligned}\n\\] Proceeding as before, we have \\[\n\\begin{aligned}\n&\\begin{amatrix}{ccc|c}\n1&2&3&1\\\\\n4&5&6&2\\\\\n7&8&9&3\n\\end{amatrix}\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|c}\n1&2&3&1\\\\\n0&-3&-6&-2\\\\\n7&8&9&3\n\\end{amatrix} \\qquad R_2 \\to R_2 - 4 R_1,\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|c}\n1&2&3&1\\\\\n0&-3&-6&-2\\\\\n0&-6&-12&-4\n\\end{amatrix} \\qquad R_3 \\to R_3 - 7 R_1,\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|c}\n1&2&3&1\\\\\n0&-3&-6&-2\\\\\n0&0&0&0\n\\end{amatrix} \\qquad R_3 \\to R_3 - 2 R_2,\\\\\n\\end{aligned}\n\\] This is in row echelon form. Now the system of equations is \\[\n\\begin{aligned}\nx_1 + 2x_2 + 3x_3 &= 1,\\\\\n-3x_2-6x_3 &= -2,\\\\\n0&=0.\n\\end{aligned}\n\\] The final row doesn’t tell us anything, so there are infinitely many solutions. To proceed, and follow the usual process of working backwards to first find \\(x_3\\), then \\(x_2\\) and then \\(x_1\\), let’s say that \\(x_3=\\alpha\\), some number that we don’t know.\nThen the second equation says \\(x_2 = \\frac{2}{3} - 2x_3 = \\frac{2}{3}-2\\alpha\\), and the first equation gives \\(x_1 = 1-2x_2-3x_3=-\\frac{1}{3}+\\alpha\\). This is a solution for any value of \\(\\alpha\\), and thus there are infinitely many solutions.",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 6: Applications of matrices, and Gaussian elimination"
    ]
  },
  {
    "objectID": "week4.html",
    "href": "week4.html",
    "title": "Week 4: Introduction to matrices",
    "section": "",
    "text": "We previously saw that we can think of vectors as column vectors, that is, a series of numbers in a column: \\[\n\\begin{bmatrix} 1 \\\\ 2 \\\\ 5\\end{bmatrix}\\in\\mathbb{R}^3,\n\\] and with this idea we defined addition and scalar multiplication in the obvious ways. We can extend this idea to larger grids of numbers, for example \\[\n\\begin{bmatrix} 1 &0 &-1\\\\3 & 2 & -2 \\end{bmatrix} \\in \\mathbb{R}^{2\\times3}.\n\\] This is called a matrix, in particular a \\(2\\times 3\\) real1 matrix2. There are two rows and three columns.\nWe typically write matrices with the following notation3: \\[\nA = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21} & a_{22} \\end{bmatrix},\n\\] \\[\nB = \\begin{bmatrix} b_{11} & b_{12} \\\\ b_{21} & b_{22} \\\\ b_{31} & b_{32} \\end{bmatrix},\n\\] etc.\nCapital, non-bold letters for the matrix, and lowercase letters with row then column for the scalar entries of the matrix. The only exception to this is that when there is only one column, we usually revert to our old notation of \\({\\mathbf{c}} = \\begin{bmatrix} c_1\\\\c_2\\\\c_3\\\\c_4 \\end{bmatrix}\\), because column vectors are matrices (sometimes called column matrices).\nThink: what shape are the matrices \\(A\\), \\(B\\) and \\({\\mathbf{c}}\\) above?\nSometimes we call the shape of a matrix its order.",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 4: Introduction to matrices"
    ]
  },
  {
    "objectID": "week4.html#the-transpose",
    "href": "week4.html#the-transpose",
    "title": "Week 4: Introduction to matrices",
    "section": "The transpose",
    "text": "The transpose\nAn extra operation we use for matrices, which we use a lot, is called the transpose, denoted with a little letter T. This takes a matrix and exchanges the rows with the columns. So the transpose of a \\(5\\times 2\\) matrix is a \\(2\\times 5\\) matrix.\nIf \\[\nA=\\begin{bmatrix} 1 &3&4\\\\2&1&1 \\end{bmatrix}\n\\] its transpose is \\[\nA^T=\\begin{bmatrix} 1 &2\\\\3 & 1\\\\4&1 \\end{bmatrix}.\n\\]\nThink: what’s the transpose of \\(M=\\begin{bmatrix} 1& 2\\\\3& 4 \\end{bmatrix}\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(M^T=\\begin{bmatrix} 1 &3\\\\2&4 \\end{bmatrix}\\)\n\n\n\n\n\n\n\n\n\nCommon mistake\n\n\n\nOften people flip over the wrong diagonal. The diagonal that starts at the top left is THE diagonal of a matrix, and it doesn’t change when we transpose.\n\n\n\nProperties of the transpose\nYou can easily prove the following statements:\n\nThe transpose of a square matrix is square.\nThe transpose of the transpose is the original matrix: \\(\\left(A^T\\right)^T=A\\).\nThe transpose of a product obeys \\((AB)^T = B^TA^T\\).\nThe transpose of a sum obeys \\((A+B)^T = A^T + B^T\\).",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 4: Introduction to matrices"
    ]
  },
  {
    "objectID": "week4.html#properties-of-matrix-multiplication",
    "href": "week4.html#properties-of-matrix-multiplication",
    "title": "Week 4: Introduction to matrices",
    "section": "Properties of matrix multiplication",
    "text": "Properties of matrix multiplication\nIt’s straightforward, if boring, to prove the following:\n\n\\(A(B+C) = AB+AC\\): it’s distributive over addition\n\\((AB)C = A(BC)\\): it’s associative.\nIt’s not commutative. \\(AB\\) is not the same as \\(BA\\). One may be possible and the other not possible.\nFor any matrix \\(A\\), and the identity matrix \\(I\\) (of the correct size for the multiplication to work) \\(AI = A\\) and \\(IA = A\\).",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 4: Introduction to matrices"
    ]
  },
  {
    "objectID": "week4.html#properties-of-the-inverse",
    "href": "week4.html#properties-of-the-inverse",
    "title": "Week 4: Introduction to matrices",
    "section": "Properties of the inverse",
    "text": "Properties of the inverse\nUsing arguments similar to that above, you could prove the following for \\(2\\times2\\) inverses:4\n\nThe inverse of a square matrix is square.\nIf the inverse exists, it is both a right- and left- inverse, even though matrix multiplication is not commutation. So \\(AA^{-1}=I\\) means \\(A^{-1}A = I\\).\nThe inverse is unique. If \\(AB=I\\), then \\(B=A^{-1}\\) and \\(A=B^{-1}\\).\nThe inverse of the inverse is the original matrix: \\(\\left(A^{-1}\\right)^{-1}=A\\).\nThe inverse of the transpose is the transpose of the inverse: \\(\\left(A^T\\right)^{-1}=\\left(A^{-1}\\right)^T\\).\nThe inverse of a product obeys \\((AB)^{-1} = B^{-1}A^{-1}\\).",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 4: Introduction to matrices"
    ]
  },
  {
    "objectID": "week4.html#footnotes",
    "href": "week4.html#footnotes",
    "title": "Week 4: Introduction to matrices",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nEventually you’ll see matrices with complex numbers inside.↩︎\nPronounced “two by three”.↩︎\nAgain, many authors use round brackets. You can too if you like.↩︎\nIn fact they hold for any sized square matrix.↩︎",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 4: Introduction to matrices"
    ]
  },
  {
    "objectID": "week2.html#unit-vectors",
    "href": "week2.html#unit-vectors",
    "title": "Week 2: General Vectors",
    "section": "Unit vectors",
    "text": "Unit vectors\nA unit vector is a vector with magnitude equal to 1. We sometimes use a little hat to denote unit vectors, like \\(\\hat{{\\mathbf{a}}}\\). So \\(|\\hat{{\\mathbf{a}}}| = 1\\).\nSuppose you want to find a unit vector in the direction of another vector \\({\\mathbf{a}}\\). Well \\({\\mathbf{b}} = \\lambda {\\mathbf{a}}\\) is a vector in the right direction. Using the rules above, it has magnitude \\(|{\\mathbf{b}}|=|\\lambda{\\mathbf{a}}| = |\\lambda| |{\\mathbf{a}}|\\). So if we choose \\(\\lambda = \\frac{1}{|{\\mathbf{a}}|}\\), we have \\(|{\\mathbf{b}}| = \\frac{1}{|{\\mathbf{a}}|} |{\\mathbf{a}}| = 1\\). So \\({\\mathbf{b}}\\) is a unit vector in the direction of \\({\\mathbf{a}}\\).\n\n\n\n\n\n\nNote\n\n\n\nTo turn any vector into a unit vector, multiply it by one divided by the magnitude.",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 2: General Vectors"
    ]
  },
  {
    "objectID": "week2.html#position-vectors",
    "href": "week2.html#position-vectors",
    "title": "Week 2: General Vectors",
    "section": "Position vectors",
    "text": "Position vectors\nSo far we’ve discussed points in space. If we choose a special point, called the origin \\(O\\), then we can define the position of any other point relative to this.\nSo given points \\(A\\) and \\(B\\), instead of just thinking about the displacement between them, if we choose an origin we can use vectos \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\) to denote their positions relative to the origin.\n\n\n\n\n\n\n\nNote\n\n\n\nThe position vector of a point is just its displacement from the origin, \\({\\mathbf{a}}={\\overrightarrow{OA}}\\).\n\n\nThink: what is the displacement vector \\({\\overrightarrow{AB}}\\) in terms of the position vectors \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe know that \\({\\mathbf{a}}={\\overrightarrow{OA}}\\) and \\({\\mathbf{b}} = {\\overrightarrow{OB}}\\). Remember from our rules for displacements that \\({\\overrightarrow{AB}} = {\\overrightarrow{AO}}+{\\overrightarrow{OB}} = -{\\overrightarrow{OA}} + {\\overrightarrow{OB}}\\). So \\({\\overrightarrow{OA}} = -{\\mathbf{a}} + {\\mathbf{b}} = {\\mathbf{b}} - {\\mathbf{a}}\\).\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo get the displacement vector in terms of position vectors, subtract the position vector of the start from the position vector of the end.\n\n\n\n\n\n\n\n\nCommon mistake\n\n\n\nFor some reason people like to say that \\({\\overrightarrow{AB}}={\\mathbf{a}}+{\\mathbf{b}}\\). This is clearly wrong. Remember that \\({\\overrightarrow{BA}}=-{\\overrightarrow{AB}}\\).\n\n\nLater in the module, we’ll want to talk about a position vector for a general point in space. For historical reasons, this is usually written as \\({\\mathbf{r}}\\). Think of \\({\\mathbf{r}}\\) as an arrow pointing from the origin \\(O\\) to wherever you currently are.",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 2: General Vectors"
    ]
  },
  {
    "objectID": "week2.html#basis-vectors",
    "href": "week2.html#basis-vectors",
    "title": "Week 2: General Vectors",
    "section": "Basis vectors",
    "text": "Basis vectors\nIn two dimensions, you can write any vector as a linear combination of any two other vectors that aren’t parallel.2\nThat means, if we have \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\) and these aren’t parallel, so there is no \\(\\lambda\\) such that \\({\\mathbf{a}}=\\lambda{\\mathbf{b}}\\), then for any vector \\({\\mathbf{c}}\\), we can find \\(\\alpha\\) and \\(\\beta\\) such that \\({\\mathbf{c}}=\\alpha{\\mathbf{a}}+\\beta{\\mathbf{b}}\\).\n\n\n\n\n\n\nNote\n\n\n\nWhen doing vector problems, choose two non-parallel vectors and write all the other vectors in terms of these.\n\n\nIt’s extra helpful if we choose two vectors that are orthogonal (at right angles) and unit vectors. Normally we choose a vector of length 1 in the horizontal direction and call this \\(\\hat{{\\mathbf{x}}}\\) and a vector of length 1 in the vertical direction and call this \\(\\hat{{\\mathbf{y}}}\\).\nThen for any vector \\({\\mathbf{a}}\\), we can find unique scalar numbers \\(a_1\\) and \\(a_2\\) such that \\({\\mathbf{a}} = a_1 \\hat{{\\mathbf{x}}} + a_2 \\hat{{\\mathbf{y}}}\\).\nIn fact, we do this so often that we introduce a new notation: \\[{\\mathbf{a}} = \\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix}.\\] 3\nAnd the unit basis vectors are simply \\[\\hat{{\\mathbf{x}}} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\] and \\[\\hat{{\\mathbf{y}}} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\\]\nIn three dimensions we write \\[{\\mathbf{b}} = b_1 \\hat{{\\mathbf{x}}} + b_2 \\hat{{\\mathbf{y}}} + b_3 \\hat{{\\mathbf{z}}} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3\\end{bmatrix}.\\]\n\n\n\n\n\n\nNote\n\n\n\nVectors are not the same as coordinates! They are related but conceptually different. The position vector for the coordinates \\((3,4,5)\\) is \\(\\begin{bmatrix} 3 \\\\ 4 \\\\ 5\\end{bmatrix}\\), and we use the two different notations to distinguish them.\n\n\nFor the general position vector, the position in 3D space is given by the coordinates \\((x,y,z)\\) so \\({\\mathbf{r}} = \\begin{bmatrix} x \\\\ y \\\\ z\\end{bmatrix}\\).",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 2: General Vectors"
    ]
  },
  {
    "objectID": "week2.html#column-vectors",
    "href": "week2.html#column-vectors",
    "title": "Week 2: General Vectors",
    "section": "Column vectors",
    "text": "Column vectors\nWith this new notation, everything behaves exactly as you would expect:\n\\[\\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ -3 \\end{bmatrix} = \\begin{bmatrix} 5+1 \\\\ 2-3 \\end{bmatrix} = \\begin{bmatrix} 6 \\\\ -1 \\end{bmatrix}.\\]\n\\[4\\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} 4\\times 5 \\\\ 4\\times 2 \\end{bmatrix} = \\begin{bmatrix} 20 \\\\ 8 \\end{bmatrix}.\\]\nIf you’re unsure about any of these rules, you can rewrite \\(\\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\\) as \\(5\\hat{{\\mathbf{x}}}+2\\hat{{\\mathbf{y}}}\\) etc. and apply the list of rules above.\nOften we use the mathematical notation that \\[\\begin{bmatrix} -1 \\\\ 3 \\\\ -3\\end{bmatrix}\\in\\mathbb{R}^3.\\] This says that the vector lives in the set of obejcts which can be written as three real numbers. Obviously for 2D we write \\(\\mathbb{R}^2\\) instead.",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 2: General Vectors"
    ]
  },
  {
    "objectID": "week2.html#pythagorass-rule",
    "href": "week2.html#pythagorass-rule",
    "title": "Week 2: General Vectors",
    "section": "Pythagoras’s rule",
    "text": "Pythagoras’s rule\nWe haven’t yet defined the magnitude of a general vector. What is the magnitude of \\(\\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\\)?\nFrom the rules so far, we know that \\[ \\left|\\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\\right| = \\left|5\\hat{{\\mathbf{x}}}+2\\hat{{\\mathbf{y}}}\\right| \\leq |5\\hat{{\\mathbf{x}}}| + |2\\hat{{\\mathbf{y}}}| = 5|\\hat{{\\mathbf{x}}}| + 2|\\hat{{\\mathbf{y}}}| = 5\\times1 + 2\\times1 = 7\\] but that just tells us that \\(\\left|\\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\\right|\\leq 7\\). This isn’t enough.\nActually there are lots of possible choices. In this module, we focus on normal, ‘flat’, Euclidean space: \\[\\left|\\begin{bmatrix} 5 \\\\ 2 \\end{bmatrix}\\right| = \\sqrt{5^2 + 2^2} = \\sqrt{29}\\] which is less than 7, as expected.\nThen general rule is: \\[|{\\mathbf{a}}| = \\sqrt{a_1^2 + a_2^2}\\] and in 3D \\[|{\\mathbf{b}}| = \\sqrt{b_1^2 + b_2^2 + b_3^2}\\].\nTo convince yourself these are good definition, draw a diagram and use Pythagoras’s theorem.\n\n\n\n\n\n\nNote\n\n\n\nThe magnitude of a vector is the square root of the components squared and added up.\n\n\nThink: what is the magnitude of the vector \\(\\begin{bmatrix} 2 \\\\ -6 \\\\ -3\\end{bmatrix}\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\sqrt{2^2 + (-6)^2 + (-3)^2} = \\sqrt{4 + 36 + 9} = \\sqrt{49} = 7.\\]\nDon’t let the negatives confuse you!",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 2: General Vectors"
    ]
  },
  {
    "objectID": "week2.html#examples-for-general-vectors",
    "href": "week2.html#examples-for-general-vectors",
    "title": "Week 2: General Vectors",
    "section": "Examples for general vectors",
    "text": "Examples for general vectors\n\nExample 1\nLet \\(ABCDEF\\) be a regular hexagon with sides of length 1. Let \\(\\hat{{\\mathbf{v}}}\\) and \\(\\hat{{\\mathbf{w}}}\\) correspond to 2 adjacent sides, \\({\\overrightarrow{AB}} = \\hat{{\\mathbf{v}}}\\), \\({\\overrightarrow{AF}} = \\hat{{\\mathbf{w}}}\\) (where \\(A\\) is the bottom left point, and \\(A,B,C....\\) proceed anti-clockwise.)\nExpress all other sides and the diagonals in terms of \\(\\hat{{\\mathbf{v}}}\\) and \\(\\hat{{\\mathbf{w}}}\\). Also, find a unit vector in the direction \\(\\hat{{\\mathbf{w}}}-\\hat{{\\mathbf{v}}}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nSides: \\({\\overrightarrow{AB}}={\\overrightarrow{ED}}=\\hat{{\\mathbf{v}}}\\), \\(\\qquad{\\overrightarrow{AF}}= {\\overrightarrow{CD}}=\\hat{{\\mathbf{w}}}\\), \\(\\qquad{\\overrightarrow{BC}}= {\\overrightarrow{FE}}=\\hat{{\\mathbf{v}}} + \\hat{{\\mathbf{w}}}\\).\nDiagonals: \\({\\overrightarrow{FC}}=2\\hat{{\\mathbf{v}}}\\), \\(\\qquad{\\overrightarrow{BE}}=2\\hat{{\\mathbf{w}}}\\), \\(\\qquad{\\overrightarrow{AD}}=2(\\hat{{\\mathbf{v}}} + \\hat{{\\mathbf{w}}})\\).\nWe have \\(\\hat{{\\mathbf{w}}} - \\hat{{\\mathbf{v}}} = {\\overrightarrow{BF}} = {\\overrightarrow{CE}}\\).\nThe length of \\(\\hat{{\\mathbf{w}}} - \\hat{{\\mathbf{v}}}\\) is \\(\\sqrt{3}\\), which can be obtained from triangle \\(AFB\\). Thus a unit vector in direction of \\(\\hat{{\\mathbf{w}}} - \\hat{{\\mathbf{v}}}\\) is \\[\\frac{1}{\\sqrt{3}} ( \\hat{{\\mathbf{w}}} - \\hat{{\\mathbf{v}}}).\\]\n\n\n\n\n\nExample 2\nLet the points \\(A\\) and \\(B\\) have position vectors \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\). Find the position vector of the midpoint of \\(A\\) and \\(B\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe midpoint of \\(A\\) and \\(B\\) is halfway along the line \\(AB\\). Call this point \\(C\\), and it must satisfy \\({\\overrightarrow{AC}}=\\frac{1}{2}{\\overrightarrow{AB}}\\).\nTo get to C from the origin, we can go via \\(A\\). So the position vector is \\[{\\overrightarrow{OC}} ={\\overrightarrow{OA}} + {\\overrightarrow{AC}} = {\\mathbf{a}} + \\frac{1}{2}{\\overrightarrow{AB}}.\\]\nRecall that \\({\\overrightarrow{AB}} = {\\mathbf{b}}-{\\mathbf{a}}\\) (remember this rule!), so finally \\[{\\overrightarrow{OC}} = {\\mathbf{a}} + \\frac{1}{2}\\left({\\mathbf{b}}-{\\mathbf{a}}\\right) = \\frac{1}{2}{\\mathbf{a}} + \\frac{1}{2}{\\mathbf{b}}.\\]\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe position vector of the midpoint of two points is the average of the position vectors of the two points.",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 2: General Vectors"
    ]
  },
  {
    "objectID": "week2.html#footnotes",
    "href": "week2.html#footnotes",
    "title": "Week 2: General Vectors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen writing vectors with pen and paper or on the board, bold is difficult, but there are various other possibilities and different people like different things. I use an underline. Whatever you choose, it’s very helpful to distinguish between vectors and scalars in your handwriting.↩︎\nIn three dimensions, we need to choose three vectors and so on.↩︎\nDifferent people write these vectors vertically or horizontally, with round or square brackets. It doesn’t really matter but it’s good to be consistent.↩︎",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 2: General Vectors"
    ]
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Helpful links",
    "section": "",
    "text": "https://www.3blue1brown.com/topics/linear-algebra\nThese are great videos explaining a lot of the material in this module, along with some stuff you definitely don’t need.\nhttps://phet.colorado.edu/en/simulations/vector-addition\nSome interactive demos to play with vectors.\nhttps://www.youtube.com/playlist?list=PL49CF3715CB9EF31D\nClassic lectures from MIT which go deeper but are very relevant to the matrix part of this module.\nhttps://www.cs.ox.ac.uk/files/12921/book.pdf\nA free textbook covering a lot of the module.\nhttps://www.3blue1brown.com/lessons/ldm-complex-numbers and https://www.3blue1brown.com/lessons/ldm-eulers-formula\nLong-form videos relevant to the final part of the module.",
    "crumbs": [
      "Helpful links"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MA12001 Algebra",
    "section": "",
    "text": "This webpage contains the lecture notes for the algebra half of the Mathematics 1B (MA12001) module for University of Dundee.\n\nIntroduction\nThe module MA12001 is split into two halves. This half, which we call “algebra” basically because it isn’t “calculus”, covers several areas. First we’ll see the idea of vectors, which will be familiar to many of you. Then we’ll talk about matrices, which are a way to discuss transformations of vectors. This is a first introduction to a large and important area of mathematics called linear algebra. In the final section of the module we’ll use what we’ve learnt to describe different geometric objects like lines, planes and spheres.\nThese notes contain all the examinable material within this half of the module, and maybe also some bonus stuff, which will be clearly marked as non-examinable. For details of the organisation of the module this year, please see My Dundee.\nI’ll be updating these notes weekly as we go through, and at the end of the semester I’ll make the full set of these notes available as a PDF. If you’d like to read ahead, last year’s full notes are available on MyDundee.\nIt’s very likely that there are mistakes in these notes. If you find one, please let me know ASAP at mailto:jparker002@dundee.ac.uk, no matter how trivial you think the mistake is.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "Week 1: Displacement Vectors",
    "section": "",
    "text": "The word “vector” can be used to mean slightly different but closely related concepts. Here we use a definition based on physics, and later we will see how this relates to the more general and precise mathematical definition, and also the concept of vectors in computer science.\nScalars are mathematical objects that can be described by a single number. Common examples from physics include temperature, pressure and density. Vectors are defined by two things: a direction and a magnitude. Common examples are displacement, force, acceleration and velocity 1.\nA vector space is associated with a dimension. They can have any integer number of dimensions or even be infinite-dimensional, but in this module we will consider only two- and three-dimensional vectors, which are the ones that we can easily imagine and draw.\nIn this section of the course we will examine the mathematics of vectors.",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 1: Displacement Vectors"
    ]
  },
  {
    "objectID": "week1.html#equality",
    "href": "week1.html#equality",
    "title": "Week 1: Displacement Vectors",
    "section": "Equality",
    "text": "Equality\nTwo displacements are equal if their lengths and directions are equal.\nIn the diagram Figure 1 below, \\({\\overrightarrow{AD}} = {\\overrightarrow{BC}}\\) because the directions and lengths are the same, even though the endpoints are different. On the other hand, \\({\\overrightarrow{AD}} \\ne {\\overrightarrow{DA}}\\) because even though the lengths are the same, \\(|{\\overrightarrow{AD}}| = |{\\overrightarrow{DA}}|\\), the directions are not.\nThink: is \\({\\overrightarrow{AB}}\\) equal to \\({\\overrightarrow{CD}}\\) or \\({\\overrightarrow{DC}}\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\({\\overrightarrow{CD}}\\) has the opposite direction. \\({\\overrightarrow{AB}} = {\\overrightarrow{DC}}\\).\n\n\n\n\n\n\n\n\n\nFigure 1: A parallelogram \\(ABCD\\)",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 1: Displacement Vectors"
    ]
  },
  {
    "objectID": "week1.html#adding-displacements",
    "href": "week1.html#adding-displacements",
    "title": "Week 1: Displacement Vectors",
    "section": "Adding displacements",
    "text": "Adding displacements\nAdding displacements is easy when the endpoint of the first displacement is the startpoint of the second. Consider this diagram:\n\n\n\n\n\n\nFigure 2: A triangle \\(ADF\\)\n\n\n\nHere we see that going from \\(D\\) to \\(A\\) and then from \\(A\\) to \\(F\\) is the same as going directly from \\(D\\) to \\(F\\). So we write \\({\\overrightarrow{DA}} + {\\overrightarrow{AF}} = {\\overrightarrow{DF}}\\).\nWithout knowing anything about the points \\(X\\), \\(Y\\) and \\(Z\\), we can say that \\({\\overrightarrow{XY}}+{\\overrightarrow{YZ}}={\\overrightarrow{XZ}}\\). We just “cancel” the middle letter.\n\n\n\n\n\n\nNote\n\n\n\nTo add two displacements with a common letter in the midddle, just cancel out that letter.\n\n\nIf there is no common middle letter, then we just need to remember that the displacements do not have a fixed position – we can “move” them. Sometimes it helps if we know other equivalent displacements. For example, in Figure 1, \\({\\overrightarrow{AD}} + {\\overrightarrow{AB}} = {\\overrightarrow{AD}} + {\\overrightarrow{DC}} = {\\overrightarrow{AC}}\\).\nThink: In Figure 1, what one displacement is equal to \\({\\overrightarrow{BC}} + {\\overrightarrow{DB}}\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\({\\overrightarrow{BC}} = {\\overrightarrow{AD}}\\) so \\({\\overrightarrow{BC}} +{\\overrightarrow{DB}}= {\\overrightarrow{AD}} + {\\overrightarrow{DB}} = {\\overrightarrow{AB}}\\)\n\n\n\n\nTriangle inequality\nWhen we add displacements, the length of the result is NOT the two lengths added together, in general. It is not true that \\(|{\\overrightarrow{DA}}|+|{\\overrightarrow{AF}}|=|{\\overrightarrow{DF}}|\\). In fact, thinking about Figure 2 we can see that \\(|{\\overrightarrow{DA}}|+|{\\overrightarrow{AF}}|\\geq|{\\overrightarrow{DF}}|\\). This is called the triangle inequality.\n\n\nAddition is commutative\nIn mathematics we use special words to describe different operations. An operation is commutative if we can swap around the sides and get the same answer. For example, \\(2+5 = 5+2\\), so normal addition of scalars is commutative. That’s true of vector addition too!\nThink: of the other basic operations of scalars, \\(-\\), \\(\\times\\) and \\(\\div\\), which is commutative and which are not?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nMultiplication is commutative, but subtraction and division are not: \\(3-2=1\\) but \\(2-3=-1\\) and \\(10\\div2 = 5\\) but \\(2\\div10 = 0.2\\).\n\n\n\nIn Figure 1, \\({\\overrightarrow{AD}} + {\\overrightarrow{DC}} = {\\overrightarrow{AC}}\\), and using the fact that \\({\\overrightarrow{DC}} = {\\overrightarrow{AB}}\\) and \\({\\overrightarrow{AD}} = {\\overrightarrow{BC}}\\), we see that \\({\\overrightarrow{DC}} + {\\overrightarrow{AD}} = {\\overrightarrow{AB}} + {\\overrightarrow{BC}} = {\\overrightarrow{AC}}\\), so we have proven that \\({\\overrightarrow{AD}} + {\\overrightarrow{DC}} = {\\overrightarrow{DC}} + {\\overrightarrow{AD}}\\).\n\n\nAddition is associative\nAn operation is associative if it doesn’t matter where we write brackets. For example, \\((3+2) + 5 = 3 + (2+5)\\). For associative operations, we don’t need to write the brackets at all, and it’s unambiguous to say “three plus five plus two”. (This isn’t true of subtraction. If someone said “ten minus two minus one” would the answer be 7 or 9? \\((10-2)-1\\) is not the same as \\(10-(2-1)\\).)\nLooking at Figure 1, we see that \\(({\\overrightarrow{AB}} + {\\overrightarrow{BC}}) + {\\overrightarrow{CD}} = {\\overrightarrow{AC}} + {\\overrightarrow{CD}} = {\\overrightarrow{AD}}\\), and \\({\\overrightarrow{AB}} + ({\\overrightarrow{BC}} + {\\overrightarrow{CD}}) = {\\overrightarrow{AB}} + {\\overrightarrow{BD}} = {\\overrightarrow{AD}}\\). Therefore, we have proven that \\(({\\overrightarrow{AB}} + {\\overrightarrow{BC}}) + {\\overrightarrow{CD}} = {\\overrightarrow{AB}} + ({\\overrightarrow{BC}} + {\\overrightarrow{CD}})\\). Addition of displacements is associative.\n\n\nThe zero displacement\nWith normal scalar numbers, we have the special number \\(0\\) which we can add to anything and not change the answer. \\(3+0=3\\), \\(0+10=10\\), etc.\nWe define a special displacement, denoted \\({\\mathbf{0}}\\), which we can add to any displacement and not change the answer. \\({\\overrightarrow{AB}} + {\\mathbf{0}} = {\\overrightarrow{AB}}\\), and also, by commutativity, \\({\\mathbf{0}} + {\\overrightarrow{AB}} = {\\overrightarrow{AB}}\\). We think of this displacement as starting at any point, and then not moving!\n\n\nNegative displacements\nEvery normal positive number has a corresponding negative number, and these have the special property that they add together to give \\(0\\). That is to say, \\(3+(-3) = 0\\).\nWe would like to have the same thing with displacements. What displacement can I add to \\({\\overrightarrow{AB}}\\) to get the zero displacement, i.e. to get back to where I started? Obviously, we should choose \\({\\overrightarrow{BA}}\\). So, \\({\\overrightarrow{AB}} + {\\overrightarrow{BA}} = {\\mathbf{0}}\\), and we write that \\(-{\\overrightarrow{BA}} = {\\overrightarrow{AB}}\\).\n\n\n\n\n\n\nNote\n\n\n\nTo take the negative of a displacement, just swap the letters round.\n\n\n\n\nSubtracting displacements\nWe define subtraction using the negatives in the obvious way, so \\({\\overrightarrow{AB}} - {\\overrightarrow{CB}} = {\\overrightarrow{AB}} + (-{\\overrightarrow{CB}}) = {\\overrightarrow{AB}} + {\\overrightarrow{BC}}\\).",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 1: Displacement Vectors"
    ]
  },
  {
    "objectID": "week1.html#multiplying-displacements",
    "href": "week1.html#multiplying-displacements",
    "title": "Week 1: Displacement Vectors",
    "section": "Multiplying displacements",
    "text": "Multiplying displacements\nFor now, let’s ignore the idea of multiplying two displacements together, and instead concentrate on multiplying displacements by scalars.\nWe can define multiplication by an integer in an obvious way: \\[\n3{\\overrightarrow{AB}} = {\\overrightarrow{AB}} + {\\overrightarrow{AB}} + {\\overrightarrow{AB}}.\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nFor normal numbers \\(ab\\), \\(a\\times b\\) and \\(a\\cdot b\\) are all the same thing. For vectors, we save the dot and cross for special meanings, and only ever use the first notation for multiplication of a scalar and a vector.\n\n\nHere the result of \\(3{\\overrightarrow{AB}}\\) is clearly in the same direction as \\({\\overrightarrow{AB}}\\), and it has three times the length. So in general, multiplying by a scalar results in a displacement in the same direction, but with the length changed.\n\n\n\n\n\n\nNote\n\n\n\nTo multiply a displacement by a negative number, first multiply by the positive number and then flip the direction.\n\n\n\nScalar multiplication is distributive over vector addition\nNow for some more jargon: an operation are distributive over another operation if we can expand out the brackets. So for normal multiplication and addition, \\(3\\times(5+2) = 3\\times 5+3\\times 2\\).\nWhat about for displacments? \\[\n2{\\overrightarrow{AB}} + 2{\\overrightarrow{XY}} = {\\overrightarrow{AB}} + {\\overrightarrow{AB}} + {\\overrightarrow{XY}} + {\\overrightarrow{XY}} = ({\\overrightarrow{AB}} + {\\overrightarrow{XY}}) + ({\\overrightarrow{AB}} + {\\overrightarrow{XY}}) = 2({\\overrightarrow{AB}}+{\\overrightarrow{XY}}).\n\\]\nThink: which of the other rules have we used in this derivation?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTo write \\({\\overrightarrow{AB}} + {\\overrightarrow{AB}} + {\\overrightarrow{XY}} + {\\overrightarrow{XY}} = {\\overrightarrow{AB}} + {\\overrightarrow{XY}} + {\\overrightarrow{AB}} + {\\overrightarrow{XY}}\\), we have used commutativity of addition. To add the brackets in, we used associativity.\n\n\n\n\n\n\n\n\n\nFigure 3: The vector \\({\\overrightarrow{AC}}\\) is twice the length of \\({\\overrightarrow{AB}}\\) and in the same direction, so \\({\\overrightarrow{AC}}=2{\\overrightarrow{AB}}\\).\n\n\n\nIn Figure 3, we see that \\({\\overrightarrow{BC}}={\\overrightarrow{AB}}\\), so \\({\\overrightarrow{AC}}={\\overrightarrow{AB}}+{\\overrightarrow{BC}}=2{\\overrightarrow{AB}}\\).",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 1: Displacement Vectors"
    ]
  },
  {
    "objectID": "week1.html#dividing-by-a-scalar",
    "href": "week1.html#dividing-by-a-scalar",
    "title": "Week 1: Displacement Vectors",
    "section": "Dividing by a scalar",
    "text": "Dividing by a scalar\nWe’ve defined addition, subtraction and multiplication. What about division? To divide a vector by \\(2\\), just multiply by \\(\\frac{1}{2}\\).",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 1: Displacement Vectors"
    ]
  },
  {
    "objectID": "week1.html#colinear-points",
    "href": "week1.html#colinear-points",
    "title": "Week 1: Displacement Vectors",
    "section": "Colinear points",
    "text": "Colinear points\nThree (or more) points are colinear (they lie on the same line) if all the vectors between them are in the same or opposite directions, i.e. they can all be written as scalar multiples of each other.\nIn Figure 3, points \\(A\\), \\(B\\) and \\(C\\) are colinear because \\({\\overrightarrow{AC}}=2{\\overrightarrow{AB}}\\), or alternatively \\({\\overrightarrow{CA}}=2{\\overrightarrow{CB}}\\).",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 1: Displacement Vectors"
    ]
  },
  {
    "objectID": "week1.html#examples-for-displacements",
    "href": "week1.html#examples-for-displacements",
    "title": "Week 1: Displacement Vectors",
    "section": "Examples for displacements",
    "text": "Examples for displacements\n\nExample: Colinear points\nLet \\(A,B,C\\) be colinear, and \\({\\overrightarrow{AB}} = \\alpha {\\overrightarrow{AC}}\\), for \\(0 &lt; \\alpha &lt; 1\\).\nAlso, let \\(|{\\overrightarrow{AB}}| = \\lambda\\) and \\(|{\\overrightarrow{BC}}| = \\mu\\).\nFind \\(\\alpha\\) in terms of \\(\\lambda\\) and \\(\\mu\\).\n\n\n\n\n\n\nFigure 4\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFrom Figure 4 we see that \\(|{\\overrightarrow{AC}}| = \\lambda+ \\mu\\). What is \\(\\alpha\\)?\nNow \\(| {\\overrightarrow{AB}} | = \\alpha |{\\overrightarrow{AC}}|\\), so that Thus \\({\\overrightarrow{AB}}= \\left(\\frac{\\lambda}{\\lambda+ \\mu}\\right) {\\overrightarrow{AC}}\\). Similary \\({\\overrightarrow{BC}}= \\left(\\frac{\\mu}{\\lambda+ \\mu}\\right) {\\overrightarrow{AC}}\\).\n\n\n\n\n\nExample: Parallelogram\nLet \\(RSTU\\) be a parallelogram (see Figure 5), and extend \\(RU\\) to \\(A\\) so that \\({\\overrightarrow{RU}} = {\\overrightarrow{UA}}\\). Also let \\(C\\) be the mid-point of \\(UT\\). Prove that \\(C\\) is the midpoint of \\(SA\\).\n\n\n\n\n\n\nFigure 5\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe have \\[{\\overrightarrow{SA}} = {\\overrightarrow{SR}} + {\\overrightarrow{RA}} = {\\overrightarrow{TU}} + {\\overrightarrow{RA}} = 2\\,{\\overrightarrow{CU}} + 2\\, {\\overrightarrow{UA}} = 2 ({\\overrightarrow{CU}}+{\\overrightarrow{UA}})= 2\\, {\\overrightarrow{CA}}.\n\\] Thus \\(S,C,A\\) are colinear, and \\(C\\) is the mid-point of \\(SA\\).",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 1: Displacement Vectors"
    ]
  },
  {
    "objectID": "week1.html#footnotes",
    "href": "week1.html#footnotes",
    "title": "Week 1: Displacement Vectors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn common language, we use “speed” and “velocity” interchangeably. In physics, velocity is a vector quantity and speed is a scalar, the magnitude of the velocity.↩︎",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 1: Displacement Vectors"
    ]
  },
  {
    "objectID": "week3.html#the-dot-product",
    "href": "week3.html#the-dot-product",
    "title": "Week 3: Dot and cross products",
    "section": "The dot product",
    "text": "The dot product\nThe first type of multiplication is called the scalar product, because the result is a scalar. We will usually call it the dot product and you might also see inner product.\nThis operation is defined for \\(\\mathbb{R}^n\\) for all \\(n\\) as follows:\n\\[{\\mathbf{a}}\\cdot{\\mathbf{b}} = |{\\mathbf{a}}|\\,|{\\mathbf{b}}|\\,\\cos\\theta,\\]\nwhere \\(\\theta\\) is the angle between the two vectors.\n\n\nProperties\nFollowing the definition above and the set of rules for vectors, you can prove the following rules:\n\nIf \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\) are perpendicular, \\({\\mathbf{a}}\\cdot{\\mathbf{b}}=0\\).\n\\({\\mathbf{a}}\\cdot{\\mathbf{b}} = 0\\) tells us that the vectors are perpendicular, or one of \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\) is \\({\\mathbf{0}}\\);\n\\({\\mathbf{a}}\\cdot{\\mathbf{a}} = |{\\mathbf{a}}|^2\\);\n\\({\\mathbf{a}}\\cdot(\\lambda{\\mathbf{b}}) = \\lambda ({\\mathbf{a}}\\cdot{\\mathbf{b}})\\);\n\\({\\mathbf{a}}\\cdot{\\mathbf{b}} = {\\mathbf{b}}\\cdot{\\mathbf{a}}\\): the dot product is commutative;\n\\({\\mathbf{a}}\\cdot({\\mathbf{b}}+{\\mathbf{c}}) = {\\mathbf{a}}\\cdot{\\mathbf{b}} + {\\mathbf{a}}\\cdot{\\mathbf{c}}\\): the dot product is distributive over addition1.\n\n\n\nThe dot product with components\nA particularly important property becomes apparent when we find the dot product of vectors in components \\({\\mathbf{a}}=\\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3\\end{bmatrix}\\) and \\({\\mathbf{b}}=\\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3\\end{bmatrix}\\) using the rules above:\n\\[\n\\begin{aligned}\n{\\mathbf{a}}\\cdot{\\mathbf{b}} &= \\left(a_1 \\hat{{\\mathbf{x}}} + a_2 \\hat{{\\mathbf{y}}} + a_3\\hat{{\\mathbf{z}}}\\right)\\cdot\\left(b_1 \\hat{{\\mathbf{x}}} + b_2 \\hat{{\\mathbf{y}}} + b_3\\hat{{\\mathbf{z}}}\\right) \\\\\n&= a_1b_1 \\hat{{\\mathbf{x}}}\\cdot\\hat{{\\mathbf{x}}} + a_1b_2 \\hat{{\\mathbf{x}}}\\cdot\\hat{{\\mathbf{y}}} + a_1b_3 \\hat{{\\mathbf{x}}}\\cdot\\hat{{\\mathbf{z}}} +\na_2b_1 \\hat{{\\mathbf{y}}}\\cdot\\hat{{\\mathbf{x}}} + a_2b_2 \\hat{{\\mathbf{y}}}\\cdot\\hat{{\\mathbf{y}}} + a_2b_3 \\hat{{\\mathbf{y}}}\\cdot\\hat{{\\mathbf{z}}} \\\\ &\\;+\na_3b_1 \\hat{{\\mathbf{z}}}\\cdot\\hat{{\\mathbf{x}}} + a_3b_2 \\hat{{\\mathbf{z}}}\\cdot\\hat{{\\mathbf{y}}} + a_3b_3 \\hat{{\\mathbf{z}}}\\cdot\\hat{{\\mathbf{z}}} \\\\\n&= a_1b_1 + a_2b_2 + a_3b_3,\n\\end{aligned}\n\\]\nwhere we have used the facts that \\(\\hat{{\\mathbf{x}}}\\cdot\\hat{{\\mathbf{x}}}=\\hat{{\\mathbf{y}}}\\cdot\\hat{{\\mathbf{y}}}=\\hat{{\\mathbf{z}}}\\cdot\\hat{{\\mathbf{z}}}=1\\) and \\(\\hat{{\\mathbf{x}}}\\cdot\\hat{{\\mathbf{y}}}=\\hat{{\\mathbf{x}}}\\cdot\\hat{{\\mathbf{z}}}=\\hat{{\\mathbf{y}}}\\cdot\\hat{{\\mathbf{z}}}=0\\).\n\n\n\n\n\n\nNote\n\n\n\nTo calculate the dot product of two vectors, multiply the components together and add them up.\n\n\nThink: if \\({\\mathbf{a}}=\\begin{bmatrix} 2 \\\\ 1 \\\\ -1\\end{bmatrix}\\) and \\({\\mathbf{b}}=\\begin{bmatrix} 0 \\\\ 1 \\\\ 1\\end{bmatrix}\\), what is \\({\\mathbf{a}}\\cdot{\\mathbf{b}}\\)? What do you deduce about the vectors?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(2\\times0 + 1\\times1 + (-1)\\times1 = 0\\).\nThe dot product is zero and neither of the vectors is \\({\\mathbf{0}}\\) so the vectors are perpendicular.\n\n\n\n\n\n\n\n\n\nCommon mistake\n\n\n\nVery often people do something like \\(\\begin{bmatrix} 1 \\\\ 2 \\\\ 3\\end{bmatrix}\\cdot\\begin{bmatrix} 3 \\\\ 2 \\\\ 4\\end{bmatrix} = \\begin{bmatrix} 1\\times 3 \\\\ 2\\times 2 \\\\ 3\\times 4\\end{bmatrix}\\). This is WRONG because the result of the dot product must be a scalar, not a vector.\n\n\n\n\nExample for dot products\nShow that the diagonals of a rhombus are perpendicular.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet the corners of the rhombus be \\(A\\), \\(B\\), \\(C\\) and \\(D\\).\n\nIt’s a paralellogram, so \\({\\overrightarrow{AB}}={\\overrightarrow{DC}}\\) - let’s call this \\({\\mathbf{a}}\\) - and \\({\\overrightarrow{BC}}={\\overrightarrow{AD}}={\\mathbf{b}}\\).\nFuthermore, for a rhombus we know the side lengths are the same, so \\(|{\\mathbf{a}}| = |{\\mathbf{b}}|\\).\nWe want to show that \\({\\overrightarrow{AC}}\\cdot{\\overrightarrow{BD}} = 0\\).\nWe see from the diagram that \\({\\overrightarrow{AC}} = {\\mathbf{a}}+{\\mathbf{b}}\\) and \\({\\overrightarrow{BD}} = {\\mathbf{b}}-{\\mathbf{a}}\\). Then\n\\[\n{\\overrightarrow{AC}}\\cdot{\\overrightarrow{BD}} = ({\\mathbf{a}}+{\\mathbf{b}})\\cdot({\\mathbf{b}}-{\\mathbf{a}}) = {\\mathbf{a}}\\cdot{\\mathbf{b}} - {\\mathbf{a}}\\cdot{\\mathbf{a}} + {\\mathbf{b}}\\cdot{\\mathbf{b}} - {\\mathbf{b}}\\cdot{\\mathbf{a}} = |{\\mathbf{b}}|^2-|{\\mathbf{a}}|^2 = 0.\n\\]",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 3: Dot and cross products"
    ]
  },
  {
    "objectID": "week3.html#the-cross-product",
    "href": "week3.html#the-cross-product",
    "title": "Week 3: Dot and cross products",
    "section": "The cross product",
    "text": "The cross product\nUnlike the dot product, which is valid in any dimensions, the cross product is only defined in three dimensions.2 Also unlike the dot product, the cross product of two vectors is a vector. For this reason it’s called the vector product. It’s written with a cross \\(\\times\\).3\nThe geometric definition is as follows:\n\nIf either \\({\\mathbf{a}}\\) or \\({\\mathbf{b}}\\) is \\({\\mathbf{0}}\\), or they’re parallel, then \\({\\mathbf{a}}\\times{\\mathbf{b}}={\\mathbf{0}}\\).\nOtherwise, find a vector the is perpendicular to both \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\) using the right-hand rule:\n\n\n\n\nFigure by Izaak Neutelings, from https://tikz.net.\n\n\n\nThe magnitude of the result \\(|{\\mathbf{a}}\\times{\\mathbf{b}}| = |{\\mathbf{a}}||{\\mathbf{b}}|\\sin\\theta\\).\n\nNote the similarity to the dot product, but with sine instead of cosine, and the fact it’s a vector.\n\nProperties\nThe following properties have similarities and differences with the corresponding ones for the dot product:\n\nIf \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\) are parallel, \\({\\mathbf{a}}\\times{\\mathbf{b}}={\\mathbf{0}}\\).\n\\({\\mathbf{a}}\\times{\\mathbf{b}} = {\\mathbf{0}}\\) tells us that the vectors are parallel, or one of \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\) is \\({\\mathbf{0}}\\);\n\\({\\mathbf{a}}\\times{\\mathbf{a}} = {\\mathbf{0}}\\);\n\\({\\mathbf{a}}\\times(\\lambda{\\mathbf{b}}) = \\lambda ({\\mathbf{a}}\\times{\\mathbf{b}})\\);\n\\({\\mathbf{a}}\\times{\\mathbf{b}} = -{\\mathbf{b}}\\times{\\mathbf{a}}\\): the dot product is anticommutative;\n\\({\\mathbf{a}}\\times({\\mathbf{b}}+{\\mathbf{c}}) = {\\mathbf{a}}\\times{\\mathbf{b}} + {\\mathbf{a}}\\times{\\mathbf{c}}\\): the cross product is distributive over addition.\n\n\n\nThe cross product with components\nIf you apply the rules above, using the fact that \\(\\hat{{\\mathbf{x}}}\\times\\hat{{\\mathbf{y}}}=\\hat{{\\mathbf{z}}}\\) - which is called a right-handed coordinate system - you can prove the following important formula:\n\\[\n\\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3\\end{bmatrix} \\times \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3\\end{bmatrix} = \\begin{bmatrix} a_2b_3-a_3b_2 \\\\ a_3b_1-a_1b_3 \\\\ a_1b_2 - a_2b_1\\end{bmatrix}.\n\\]\nThis ugly formula is one of the only things in this module I recommend you learn by heart. There are patterns which can help you memorise it; try to find something that works for you.\n\n\n\n\n\n\nCommon mistake\n\n\n\nPeople mess up the second component of the cross product. It might be the negative of what you intuitively think it should be.\n\n\n\n\n\n\n\n\nNote\n\n\n\nMany people like to write the cross product as the determinant of a matrix, as in\n\\[\n\\begin{bmatrix} a_1 \\\\ a_2 \\\\ a_3\\end{bmatrix} \\times \\begin{bmatrix} b_1 \\\\ b_2 \\\\ b_3\\end{bmatrix} = \\left|\\begin{matrix} \\hat{{\\mathbf{x}}} & \\hat{{\\mathbf{y}}} & \\hat{{\\mathbf{z}}} \\\\\na_1 & a_2 & a_3 \\\\\nb_1 & b_2 & b_3\n\\end{matrix}\\right|.\n\\]\nDon’t worry if you’re not familiar with determinants (or matrices), we’ll see them in a couple of weeks.\nI think this formula is stupid anyway because you can’t put vectors inside a matrix like that. But if it works for you, feel free to use it.\n\n\nThink: if \\({\\mathbf{a}}=\\begin{bmatrix} 2 \\\\ 1 \\\\ -1\\end{bmatrix}\\) and \\({\\mathbf{b}}=\\begin{bmatrix} 0 \\\\ 1 \\\\ 1\\end{bmatrix}\\), what is \\({\\mathbf{a}}\\times{\\mathbf{b}}\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\begin{bmatrix} 1\\times 1-(-1)\\times 1 \\\\ -1\\times 0-2\\times 1 \\\\ 2\\times 1 - 1\\times 0\\end{bmatrix} = \\begin{bmatrix} 2 \\\\ -2 \\\\ 2\\end{bmatrix}.\\]\n\n\n\n\n\nApplications\nYou may remember the formula for the area of a triangle as \\(\\frac{1}{2} b c \\sin \\alpha\\) where \\(\\alpha\\) is the angle between the sides of length \\(b\\) and \\(c\\). Looking at the definition of the cross product, we see that\n\\[\\frac{1}{2}|{\\mathbf{a}}\\times{\\mathbf{b}}|\\]\nis the area of the triangle between \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\). Similarly, \\(|{\\mathbf{a}}\\times{\\mathbf{b}}|\\) is the area of the parallelogram with sides \\({\\mathbf{a}}\\) and \\({\\mathbf{b}}\\):\n\n\n\nExample for cross products\nFind a unit vector that is perpendicular to both \\({\\mathbf{a}}=\\begin{bmatrix} 3 \\\\ 1 \\\\ 2\\end{bmatrix}\\) and \\({\\mathbf{b}}=\\begin{bmatrix} 1 \\\\ 1 \\\\ -1\\end{bmatrix}\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe know that the result of the cross product \\({\\mathbf{a}}\\times{\\mathbf{b}}\\) is perpendicular to both of them:\n\\[\\begin{bmatrix} 3 \\\\ 1 \\\\ 2\\end{bmatrix}\\times\\begin{bmatrix} 1 \\\\ 1 \\\\ -1\\end{bmatrix} = \\begin{bmatrix} -3 \\\\ 5 \\\\ 2\\end{bmatrix}.\\]\nThe question asks for a unit vector, so we need to divide this vector by its own magnitude. The magnitude is\n\\[\\sqrt{(-3)^2+5^2+2^2} = \\sqrt{38}\\]\nand so the final answer is\n\\[\\begin{bmatrix} \\frac{-3}{\\sqrt{38}} \\\\ \\frac{5}{\\sqrt{38}} \\\\ \\frac{2}{\\sqrt{38}}\\end{bmatrix}.\\]\nThe vector opposite to this, i.e. \\(\\begin{bmatrix} \\frac{3}{\\sqrt{38}} \\\\ \\frac{-5}{\\sqrt{38}} \\\\ \\frac{-2}{\\sqrt{38}}\\end{bmatrix}\\), would also satisfy the requirements.",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 3: Dot and cross products"
    ]
  },
  {
    "objectID": "week3.html#footnotes",
    "href": "week3.html#footnotes",
    "title": "Week 3: Dot and cross products",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is easiest to prove using the component form of the dot product, but unfortunately that gives a circular proof. To prove it directly requires a lot of geometry and trigonometry.↩︎\nSometimes people define a cross product in two dimensions by imagining the 2d vectors on a plane in 3d space, but we won’t use that definition.↩︎\nOccasionally you see people use \\(\\wedge\\) instead. In modern maths this the exterior product, which is a related but different concept.↩︎",
    "crumbs": [
      "Chapter 1: Vectors",
      "Week 3: Dot and cross products"
    ]
  },
  {
    "objectID": "week5.html",
    "href": "week5.html",
    "title": "Week 5: Determinants and inverses of \\(3\\times 3\\) matrices",
    "section": "",
    "text": "\\[\n\\newenvironment{amatrix}[1]{%\n  \\left[\\begin{array}{#1}\n}{%\n  \\end{array}\\right]\n}\n\\]\nLast week, we saw that for a matrix \\[\nA = \\begin{bmatrix} a&b\\\\c&d \\end{bmatrix},\n\\] the inverse is \\[\nA^{-1} = \\frac{1}{ad-bc} \\begin{bmatrix} d &-b\\\\ -c & a \\end{bmatrix}.\n\\]\nThis inverse exists if and only if \\(ad-bc\\neq 0\\). We call this quantity \\(ad-bc\\) the determinant, and write \\[\\det{A}=ad-bc.\\] You can think of the determinant as a bit like the discriminant for quadratic equations. It determines whether the matrix is invertible or singular.\nSometimes we also write the determinant as \\(|A|\\), but I’ll avoid this notation because it’s different from the absolute value of a scalar or magnitude of a vector. The determinant can be negative.",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 5: Determinants and inverses of $3\\times 3$ matrices"
    ]
  },
  {
    "objectID": "week5.html#example",
    "href": "week5.html#example",
    "title": "Week 5: Determinants and inverses of \\(3\\times 3\\) matrices",
    "section": "Example",
    "text": "Example\nIs the matrix \\[\nA=\\begin{bmatrix} 1&2&3\\\\3&6&9\\\\2&1&1 \\end{bmatrix}\n\\] invertible? Justify your answer.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we do the full determinant calculation we find that \\[\n\\det A = 0,\n\\] so it is singular, it is not invertible.",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 5: Determinants and inverses of $3\\times 3$ matrices"
    ]
  },
  {
    "objectID": "week5.html#swapping-two-rows-in-a-matrix",
    "href": "week5.html#swapping-two-rows-in-a-matrix",
    "title": "Week 5: Determinants and inverses of \\(3\\times 3\\) matrices",
    "section": "Swapping two rows in a matrix",
    "text": "Swapping two rows in a matrix\nIf we left-multiply a \\(3\\times3\\) matrix by the matrix \\(\\begin{bmatrix} 1 & 0 &0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}\\), the result is that the second and third rows get swapped: \\[\n\\begin{bmatrix} 1 & 0 &0 \\\\ 0 & 0 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}\\begin{bmatrix} 1 & 2 & 3\\\\4&5&6\\\\7&8&9 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 3\\\\7&8&9\\\\4&5&6 \\end{bmatrix}.\n\\] Go through this multiplication slowly to convince yourself.\nThink: what matrix would swap the first and third rows and leave the second row as it is?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{bmatrix} 0&0&1\\\\0 & 1 & 0\\\\1&0&0 \\end{bmatrix}.\n\\]",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 5: Determinants and inverses of $3\\times 3$ matrices"
    ]
  },
  {
    "objectID": "week5.html#multiplying-a-row-by-a-scalar",
    "href": "week5.html#multiplying-a-row-by-a-scalar",
    "title": "Week 5: Determinants and inverses of \\(3\\times 3\\) matrices",
    "section": "Multiplying a row by a scalar",
    "text": "Multiplying a row by a scalar\nIf we left-multiply a \\(3\\times3\\) matrix by the matrix \\(\\begin{bmatrix} 1 & 0 &0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0.5 \\end{bmatrix}\\), the result is that the third row is multiplied by \\(0.5\\): \\[\n\\begin{bmatrix} 1 & 0 &0 \\\\ 0 &1 & 0 \\\\ 0 & 0 & 0.5 \\end{bmatrix}\\begin{bmatrix} 1 & 2 & 3\\\\4&5&6\\\\7&8&9 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 3\\\\4&5&6\\\\3.5&4&4.5 \\end{bmatrix}.\n\\] Go through this multiplication slowly to convince yourself.\nThink: what matrix would multiply the first row by 3?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{bmatrix} 3&0&0\\\\0&1&0\\\\0&0&1 \\end{bmatrix}.\n\\]",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 5: Determinants and inverses of $3\\times 3$ matrices"
    ]
  },
  {
    "objectID": "week5.html#adding-a-multiple-of-one-row-to-another-row",
    "href": "week5.html#adding-a-multiple-of-one-row-to-another-row",
    "title": "Week 5: Determinants and inverses of \\(3\\times 3\\) matrices",
    "section": "Adding a multiple of one row to another row",
    "text": "Adding a multiple of one row to another row\nIf we left-multiply a \\(3\\times3\\) matrix by the matrix \\(\\begin{bmatrix} 1 & 0 &0 \\\\ 0 & 1 & 0 \\\\ 2 & 0 & 1 \\end{bmatrix}\\), the result is that we add \\(2\\times\\) the first row to the third row: \\[\n\\begin{bmatrix} 1 & 0 &0 \\\\ 0 &1 & 0 \\\\ 1 & 0 & 1 \\end{bmatrix}\\begin{bmatrix} 1 & 2 & 3\\\\4&5&6\\\\7&8&9 \\end{bmatrix} = \\begin{bmatrix} 1 & 2 & 3\\\\4&5&6\\\\9&12&15 \\end{bmatrix}.\n\\] Go through this multiplication slowly to convince yourself.\nThink: what matrix would add \\(4\\times\\) the first row to the second row?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\n\\begin{bmatrix} 1&0&0\\\\4&1&0\\\\0&0&1 \\end{bmatrix}.\n\\]",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 5: Determinants and inverses of $3\\times 3$ matrices"
    ]
  },
  {
    "objectID": "week5.html#the-bigger-picture",
    "href": "week5.html#the-bigger-picture",
    "title": "Week 5: Determinants and inverses of \\(3\\times 3\\) matrices",
    "section": "The bigger picture",
    "text": "The bigger picture\nWhy is this interesting? Firstly, notice that to get the matrix that performs these row oeprations, you just need to do the row operation to the identity matrix.\nSecondly suppose you have a series of row operation matrices, let’s call them \\(P_1\\), \\(P_2\\), etc., and you perform them one after another on a matrix \\(A\\):\n\\[\nP_5 P_4 P_3 P_2 P_1 A.\n\\] (Because we’re left multiplying, the order is reversed!)\nIf the result of this is the identity, then we have found the inverse of A! \\[\n(P_5 P_4 P_3 P_2 P_1) A = I\n\\] means that \\[\nA^{-1}=(P_5 P_4 P_3 P_2 P_1).\n\\] Remember that inverses are unique for square matrices, so if \\(BA=I\\) then \\(B=A^{-1}\\).",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 5: Determinants and inverses of $3\\times 3$ matrices"
    ]
  },
  {
    "objectID": "week5.html#augmented-matrices",
    "href": "week5.html#augmented-matrices",
    "title": "Week 5: Determinants and inverses of \\(3\\times 3\\) matrices",
    "section": "Augmented matrices",
    "text": "Augmented matrices\nIn order to compute \\(3\\times3\\) matrix inverses, we need to introduce a new notation, called augmented matrices. This is just two matrices with the same number of rows, stuck side-by-side:\nIf \\(A=\\begin{bmatrix} 1&2&3\\\\4&5&6\\\\7&8&9 \\end{bmatrix}\\) and \\(B=\\begin{bmatrix} 0&9&8\\\\7&6&5\\\\4&3&2 \\end{bmatrix}\\) then the augmented matrix \\([A|B]\\) is \\[\n[A|B]=\\begin{amatrix}{ccc|ccc}\n1&2&3&0&9&8\\\\4&5&6&7&6&5\\\\7&8&9&4&3&2\n\\end{amatrix}.\n\\]\nWhat this notation does is allow us to manipulate two different matrices at the same time using the same row operations, and it simplifies the whole procedure.\nFor example, taking the above matrix and subtracting \\(2\\times\\) the first row from the second row, we have\n\\[\n\\begin{aligned}\n&\\begin{amatrix}{ccc|ccc}\n1&2&3&0&9&8\\\\4&5&6&7&6&5\\\\7&8&9&4&3&2\n\\end{amatrix}\n\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|ccc}\n1&2&3&0&9&8\\\\2&1&0&7&-12&-11\\\\7&8&9&4&3&2\n\\end{amatrix} \\qquad R_2 \\to R_2 - 2 R_1\n\\end{aligned}\n\\]\nPlease make a point of writing out explicitly what row operation you’re doing so whoever’s marking your work can follow it.",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 5: Determinants and inverses of $3\\times 3$ matrices"
    ]
  },
  {
    "objectID": "week5.html#how-it-works",
    "href": "week5.html#how-it-works",
    "title": "Week 5: Determinants and inverses of \\(3\\times 3\\) matrices",
    "section": "How it works",
    "text": "How it works\nHere are the steps for finding the inverse of a \\(3\\times3\\) matrix \\(A\\):\n\nWrite an augmented matrix with the identity, i.e. \\([A|I]\\)\nPerform a series of row operations to get zeros below the diagonal and ones on the diagonal on the left hand side.\nPerform more row operations to get zeros above the diagonal.\nNow the left side of the augmented matrix should be the identity, and the right side is your inverse, \\([I|A^{-1}]\\).\n\nRemember that not all matrices have inverses. If you get stuck, it’s probably because the matrix is singular, and the determinant of the matrix will be zero. (But you could also have made a mistake, like forgetting to swap rows.)\n\n\n\n\n\n\nNote\n\n\n\nCalculate the determinant of a matrix before trying to find the inverse. If the determinant is zero, no inverse exists. If the determinant is not zero, this method will give you the inverse.",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 5: Determinants and inverses of $3\\times 3$ matrices"
    ]
  },
  {
    "objectID": "week5.html#example-1",
    "href": "week5.html#example-1",
    "title": "Week 5: Determinants and inverses of \\(3\\times 3\\) matrices",
    "section": "Example",
    "text": "Example\nFollowing the steps above, let’s find the inverse of \\[\nA=\\begin{bmatrix} 5 &1 &0\\\\5&5&-3\\\\-10&3&\\frac{9}{4} \\end{bmatrix}.\n\\] (First find the determinant to check it’s invertible!)\n\\[\n\\begin{aligned}\n&\\begin{amatrix}{ccc|ccc}\n5 &1 &0&1&0&0\\\\5&5&-3&0&1&0\\\\-10&3&\\frac{9}{4}&0&0&1\n\\end{amatrix}\n\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|ccc}\n1 &\\frac{1}{5} &0&\\frac{1}{5}&0&0\\\\5&5&-3&0&1&0\\\\-10&3&\\frac{9}{4}&0&0&1\n\\end{amatrix} \\qquad R_1 \\to \\frac{1}{5}R_1 \\qquad \\text{(to get a 1 on the diagonal entry)}\n\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|ccc}\n1 &\\frac{1}{5} &0&\\frac{1}{5}&0&0\\\\0&4&-3&-1&1&0\\\\-10&3&\\frac{9}{4}&0&0&1\n\\end{amatrix} \\qquad R_2 \\to R_2-5R_1 \\qquad \\text{(to get 0 below the diagonal)}\n\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|ccc}\n1 &\\frac{1}{5} &0&\\frac{1}{5}&0&0\\\\0&4&-3&-1&1&0\\\\0&5&\\frac{9}{4}&2&0&1\n\\end{amatrix} \\qquad R_3 \\to R_3+10R_1 \\qquad \\text{(to get 0 in the bottom left)}\n\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|ccc}\n1 &\\frac{1}{5} &0&\\frac{1}{5}&0&0\\\\0&1&-\\frac{3}{4}&-\\frac{1}{4}&\\frac{1}{4}&0\\\\0&5&\\frac{9}{4}&2&0&1\n\\end{amatrix} \\qquad R_2 \\to \\frac{1}{4}R_2 \\qquad \\text{(to get 1 on the diagonal)}\n\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|ccc}\n1 &\\frac{1}{5} &0&\\frac{1}{5}&0&0\\\\0&1&-\\frac{3}{4}&-\\frac{1}{4}&\\frac{1}{4}&0\\\\0&0&6&\\frac{13}{4}&-\\frac{5}{4}&1\n\\end{amatrix} \\qquad R_3 \\to R_3-5R_2 \\qquad \\text{(to get 0 below the diagonal)}\n\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|ccc}\n1 &\\frac{1}{5} &0&\\frac{1}{5}&0&0\\\\0&1&-\\frac{3}{4}&-\\frac{1}{4}&\\frac{1}{4}&0\\\\0&0&1&\\frac{13}{24}&-\\frac{5}{24}&\\frac{1}{6}\n\\end{amatrix} \\qquad R_3 \\to \\frac{1}{6} R_3 \\qquad \\text{(to get 1 on the diagonal)}\n\\end{aligned}\n\\]\nWe’ve now completed the second step above. We’ve worked down from the top to get ones on the diagonal and zeros below. Now for the third step we work back upwards to get zeros above the diagonal:\n\\[\n\\begin{aligned}\n&\\hookrightarrow\\begin{amatrix}{ccc|ccc}\n1 &\\frac{1}{5} &0&\\frac{1}{5}&0&0\\\\\n0&1&0&\\frac{5}{32}&\\frac{3}{32}&\\frac{1}{8}\\\\\n0&0&1&\\frac{13}{24}&-\\frac{5}{24}&\\frac{1}{6}\n\\end{amatrix} \\qquad R_2 \\to R_2 + \\frac{3}{4}R_3 \\qquad \\text{(to get 0 middle right)}\\\\\n&\\hookrightarrow\\begin{amatrix}{ccc|ccc}\n1 &0 &0&\\frac{27}{160}&-\\frac{3}{160}&-\\frac{1}{40}\\\\\n0&1&0&\\frac{5}{32}&\\frac{3}{32}&\\frac{1}{8}\\\\\n0&0&1&\\frac{13}{24}&-\\frac{5}{24}&\\frac{1}{6}\n\\end{amatrix} \\qquad R_1 \\to R_1 - \\frac{1}{5}R_2 \\qquad \\text{(to get 0 top middle)}\\\\\n\\end{aligned}\n\\]\nWe already have a zero in the top right, so we’re done! The left hand side is the identity, and the right hand side is the inverse.\n\\[\nA^{-1}=\\begin{bmatrix} \\frac{27}{160}&-\\frac{3}{160}&-\\frac{1}{40}\\\\\n\\frac{5}{32}&\\frac{3}{32}&\\frac{1}{8}\\\\\n\\frac{13}{24}&-\\frac{5}{24}&\\frac{1}{6} \\end{bmatrix}.\n\\]\nDoes it work? Calculate \\(A A^{-1}\\) and \\(A^{-1} A\\) to check.\nThere’s a general procedure here to get do divisions to get ones on the diagonal and subtraction/addition to get zeros. The problem comes when you have a zero where you need a one. In this case you have to swap rows. We’ll see examples of this in class and on the worksheets.\n\n\n\n\n\n\nNote\n\n\n\nThis process of finding the inverse is long and boring, but it’s straightforward if you take your time. Most of the marks are for understanding the method, because it’s very easy to make an arithmetic mistake.",
    "crumbs": [
      "Chapter 2: Matrices",
      "Week 5: Determinants and inverses of $3\\times 3$ matrices"
    ]
  }
]